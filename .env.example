# Environment Configuration for Phi-3 Fine-tuning
# Copy this to .env and modify as needed

# Project Configuration
PROJECT_NAME=phi3-finetune
PROJECT_ROOT=/opt/projects/phi3_finetune_new

# Model Configuration
MODEL_NAME=microsoft/Phi-3-mini-4k-instruct
MODEL_CACHE_DIR=${PROJECT_ROOT}/models/cache
CHECKPOINT_DIR=${PROJECT_ROOT}/models/checkpoints
FINAL_MODEL_DIR=${PROJECT_ROOT}/models/final

# Data Configuration
DATA_DIR=${PROJECT_ROOT}/dataset
PROCESSED_DATA_DIR=${PROJECT_ROOT}/data/processed
RESULTS_DIR=${PROJECT_ROOT}/results

# Training Configuration
MAX_SEQ_LENGTH=2048
BATCH_SIZE=1
GRADIENT_ACCUMULATION_STEPS=8
LEARNING_RATE=2e-4
NUM_EPOCHS=3
WARMUP_RATIO=0.1
WEIGHT_DECAY=0.01

# LoRA Configuration
LORA_R=16
LORA_ALPHA=32
LORA_DROPOUT=0.1
LORA_TARGET_MODULES=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]

# Quantization Configuration
USE_4BIT=true
BNB_4BIT_COMPUTE_DTYPE=float16
BNB_4BIT_QUANT_TYPE=nf4
USE_NESTED_QUANT=false

# Monitoring Configuration
LOG_DIR=${PROJECT_ROOT}/logs
WANDB_PROJECT=phi3-pii-finetune
WANDB_ENTITY=your-wandb-entity
SAVE_STEPS=100
EVAL_STEPS=100
LOGGING_STEPS=10

# Hardware Configuration
CUDA_VISIBLE_DEVICES=0
TORCH_CUDA_ARCH_LIST="8.0;8.6"  # Adjust based on your GPU architecture

# Memory Optimization
TORCH_USE_CUDA_DSA=1
PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128